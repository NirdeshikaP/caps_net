{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to laod datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the dataset\n",
    "\n",
    "First load the training, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, train_targets = load_dataset('data_clean/train')\n",
    "valid_files, valid_targets = load_dataset('data_clean/valid')\n",
    "test_files, test_targets = load_dataset('data_clean/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 classes: melanoma, nevus, seborrheic_keratosis.\n",
      "There are 2000 training images.\n",
      "There are 150 validation images.\n",
      "There are 600 testing images.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "diseases = sorted(listdir('data/train'))\n",
    "\n",
    "print('There are {} classes: {}.'.format(len(diseases), ', '.join(diseases)))\n",
    "print('There are {} training images.'.format(len(train_files)))\n",
    "print('There are {} validation images.'.format(len(valid_files)))\n",
    "print('There are {} testing images.'.format(len(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def get_tensor(path):\n",
    "    img = image.load_img(path,target_size=(100,100))\n",
    "    return np.expand_dims(image.img_to_array(img), axis=0)\n",
    "\n",
    "def get_tensors(paths):\n",
    "    return np.vstack([get_tensor(path) for path in tqdm_notebook(paths)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the Capsule Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers, models, optimizers\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCapsule(input, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap')(input)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_initializer='glorot_uniform', **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "\n",
    "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # The first two dimensions as `batch` dimension,\n",
    "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            print('Iteration')\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_capsnet_architecture(input_shape, num_class, number_of_routing_iterations):\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: ReLU Conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Primary Caps\n",
    "    primarycaps = PrimaryCapsule(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule Layer for outputs where dynamic routing works\n",
    "    tumorcaps = CapsuleLayer(num_capsule=num_class, dim_capsule=16, routings=number_of_routing_iterations,\n",
    "                             name='tumorcaps')(primarycaps)\n",
    "    \n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(tumorcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(num_class,))\n",
    "    masked_by_y = Mask()([tumorcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(tumorcaps)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(512, activation='relu', input_dim=16*num_class))\n",
    "    decoder.add(layers.Dense(1024, activation='relu'))\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "    \n",
    "    return train_model, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212e00f2389f46028c0ba7765ce314d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed9a660fe19417097d516bcb8230439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\n",
      "Iteration\n",
      "Iteration\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/2\n",
      "1984/2000 [============================>.] - ETA: 31:21 - loss: 401258.4688 - capsnet_loss: 0.7170 - decoder_loss: 26750.5156 - capsnet_acc: 0.250 - ETA: 29:25 - loss: 414696.0000 - capsnet_loss: 0.7609 - decoder_loss: 27646.3486 - capsnet_acc: 0.546 - ETA: 28:14 - loss: 400968.2188 - capsnet_loss: 0.7772 - decoder_loss: 26731.1621 - capsnet_acc: 0.520 - ETA: 27:25 - loss: 403894.8203 - capsnet_loss: 0.7854 - decoder_loss: 26926.2686 - capsnet_acc: 0.562 - ETA: 26:59 - loss: 399419.3688 - capsnet_loss: 0.7903 - decoder_loss: 26627.9047 - capsnet_acc: 0.562 - ETA: 26:28 - loss: 397058.7031 - capsnet_loss: 0.7936 - decoder_loss: 26470.5267 - capsnet_acc: 0.572 - ETA: 25:52 - loss: 397389.8438 - capsnet_loss: 0.7959 - decoder_loss: 26492.6027 - capsnet_acc: 0.602 - ETA: 25:21 - loss: 393887.8203 - capsnet_loss: 0.7977 - decoder_loss: 26259.1343 - capsnet_acc: 0.609 - ETA: 25:23 - loss: 392394.9861 - capsnet_loss: 0.7991 - decoder_loss: 26159.6120 - capsnet_acc: 0.625 - ETA: 24:59 - loss: 392607.4437 - capsnet_loss: 0.8001 - decoder_loss: 26173.7758 - capsnet_acc: 0.634 - ETA: 24:36 - loss: 393246.9545 - capsnet_loss: 0.8010 - decoder_loss: 26216.4098 - capsnet_acc: 0.656 - ETA: 24:07 - loss: 395852.1953 - capsnet_loss: 0.8018 - decoder_loss: 26390.0924 - capsnet_acc: 0.661 - ETA: 23:36 - loss: 398246.7668 - capsnet_loss: 0.8024 - decoder_loss: 26549.7305 - capsnet_acc: 0.665 - ETA: 23:06 - loss: 396915.0446 - capsnet_loss: 0.8030 - decoder_loss: 26460.9489 - capsnet_acc: 0.656 - ETA: 22:38 - loss: 394837.1937 - capsnet_loss: 0.8034 - decoder_loss: 26322.4255 - capsnet_acc: 0.660 - ETA: 22:09 - loss: 393384.4238 - capsnet_loss: 0.8038 - decoder_loss: 26225.5742 - capsnet_acc: 0.658 - ETA: 21:41 - loss: 392277.1857 - capsnet_loss: 0.8042 - decoder_loss: 26151.7583 - capsnet_acc: 0.652 - ETA: 21:25 - loss: 392812.1719 - capsnet_loss: 0.8045 - decoder_loss: 26187.4240 - capsnet_acc: 0.663 - ETA: 21:12 - loss: 394184.7582 - capsnet_loss: 0.8048 - decoder_loss: 26278.9298 - capsnet_acc: 0.666 - ETA: 20:55 - loss: 393837.4500 - capsnet_loss: 0.8051 - decoder_loss: 26255.7759 - capsnet_acc: 0.668 - ETA: 20:32 - loss: 393494.3408 - capsnet_loss: 0.8053 - decoder_loss: 26232.9020 - capsnet_acc: 0.666 - ETA: 20:08 - loss: 393897.1463 - capsnet_loss: 0.8055 - decoder_loss: 26259.7557 - capsnet_acc: 0.661 - ETA: 19:43 - loss: 393300.4674 - capsnet_loss: 0.8057 - decoder_loss: 26219.9771 - capsnet_acc: 0.665 - ETA: 19:20 - loss: 394252.0169 - capsnet_loss: 0.8059 - decoder_loss: 26283.4137 - capsnet_acc: 0.666 - ETA: 18:53 - loss: 394439.9012 - capsnet_loss: 0.8061 - decoder_loss: 26295.9394 - capsnet_acc: 0.668 - ETA: 18:26 - loss: 394759.6010 - capsnet_loss: 0.8062 - decoder_loss: 26317.2527 - capsnet_acc: 0.671 - ETA: 17:58 - loss: 396911.1863 - capsnet_loss: 0.8063 - decoder_loss: 26460.6917 - capsnet_acc: 0.672 - ETA: 17:30 - loss: 397104.0045 - capsnet_loss: 0.8065 - decoder_loss: 26473.5462 - capsnet_acc: 0.673 - ETA: 17:01 - loss: 396147.6929 - capsnet_loss: 0.8066 - decoder_loss: 26409.7922 - capsnet_acc: 0.669 - ETA: 16:32 - loss: 396222.8510 - capsnet_loss: 0.8067 - decoder_loss: 26414.8027 - capsnet_acc: 0.669 - ETA: 16:04 - loss: 396046.8397 - capsnet_loss: 0.8068 - decoder_loss: 26403.0685 - capsnet_acc: 0.671 - ETA: 15:36 - loss: 396409.7236 - capsnet_loss: 0.8069 - decoder_loss: 26427.2608 - capsnet_acc: 0.677 - ETA: 15:07 - loss: 395852.4242 - capsnet_loss: 0.8070 - decoder_loss: 26390.1075 - capsnet_acc: 0.677 - ETA: 14:37 - loss: 396395.8244 - capsnet_loss: 0.8071 - decoder_loss: 26426.3342 - capsnet_acc: 0.677 - ETA: 14:07 - loss: 396659.6964 - capsnet_loss: 0.8072 - decoder_loss: 26443.9256 - capsnet_acc: 0.676 - ETA: 13:37 - loss: 395602.1840 - capsnet_loss: 0.8073 - decoder_loss: 26373.4248 - capsnet_acc: 0.677 - ETA: 13:07 - loss: 395741.5709 - capsnet_loss: 0.8073 - decoder_loss: 26382.7173 - capsnet_acc: 0.677 - ETA: 12:37 - loss: 395415.8207 - capsnet_loss: 0.8074 - decoder_loss: 26361.0006 - capsnet_acc: 0.677 - ETA: 12:06 - loss: 396439.0986 - capsnet_loss: 0.8075 - decoder_loss: 26429.2191 - capsnet_acc: 0.680 - ETA: 11:37 - loss: 396678.9437 - capsnet_loss: 0.8075 - decoder_loss: 26445.2088 - capsnet_acc: 0.679 - ETA: 11:07 - loss: 396395.0168 - capsnet_loss: 0.8076 - decoder_loss: 26426.2803 - capsnet_acc: 0.679 - ETA: 10:36 - loss: 397781.7582 - capsnet_loss: 0.8077 - decoder_loss: 26518.7297 - capsnet_acc: 0.680 - ETA: 10:06 - loss: 398170.6817 - capsnet_loss: 0.8077 - decoder_loss: 26544.6580 - capsnet_acc: 0.681 - ETA: 9:36 - loss: 398250.3842 - capsnet_loss: 0.8078 - decoder_loss: 26549.9715 - capsnet_acc: 0.679 - ETA: 9:06 - loss: 397349.2590 - capsnet_loss: 0.8078 - decoder_loss: 26489.8964 - capsnet_acc: 0.68 - ETA: 8:36 - loss: 396681.4524 - capsnet_loss: 0.8079 - decoder_loss: 26445.3760 - capsnet_acc: 0.67 - ETA: 8:06 - loss: 397127.6396 - capsnet_loss: 0.8079 - decoder_loss: 26475.1218 - capsnet_acc: 0.68 - ETA: 7:36 - loss: 397537.5456 - capsnet_loss: 0.8079 - decoder_loss: 26502.4489 - capsnet_acc: 0.68 - ETA: 7:05 - loss: 397397.8533 - capsnet_loss: 0.8080 - decoder_loss: 26493.1361 - capsnet_acc: 0.67 - ETA: 6:35 - loss: 397380.8725 - capsnet_loss: 0.8080 - decoder_loss: 26492.0040 - capsnet_acc: 0.67 - ETA: 6:05 - loss: 396751.2114 - capsnet_loss: 0.8081 - decoder_loss: 26450.0266 - capsnet_acc: 0.67 - ETA: 5:34 - loss: 397181.4736 - capsnet_loss: 0.8081 - decoder_loss: 26478.7107 - capsnet_acc: 0.67 - ETA: 5:02 - loss: 397686.7571 - capsnet_loss: 0.8081 - decoder_loss: 26512.3963 - capsnet_acc: 0.67 - ETA: 4:31 - loss: 397520.8877 - capsnet_loss: 0.8082 - decoder_loss: 26501.3384 - capsnet_acc: 0.67 - ETA: 3:59 - loss: 397872.0949 - capsnet_loss: 0.8082 - decoder_loss: 26524.7522 - capsnet_acc: 0.67 - ETA: 3:27 - loss: 398048.5262 - capsnet_loss: 0.8082 - decoder_loss: 26536.5143 - capsnet_acc: 0.67 - ETA: 2:56 - loss: 397839.8037 - capsnet_loss: 0.8083 - decoder_loss: 26522.5995 - capsnet_acc: 0.67 - ETA: 2:24 - loss: 397985.1492 - capsnet_loss: 0.8083 - decoder_loss: 26532.2892 - capsnet_acc: 0.67 - ETA: 1:52 - loss: 397485.5143 - capsnet_loss: 0.8083 - decoder_loss: 26498.9802 - capsnet_acc: 0.67 - ETA: 1:20 - loss: 397399.8365 - capsnet_loss: 0.8084 - decoder_loss: 26493.2683 - capsnet_acc: 0.67 - ETA: 48s - loss: 397393.9109 - capsnet_loss: 0.8084 - decoder_loss: 26492.8733 - capsnet_acc: 0.6726 - ETA: 16s - loss: 396992.6557 - capsnet_loss: 0.8084 - decoder_loss: 26466.1230 - capsnet_acc: 0.6714Epoch 00001: val_loss improved from inf to 359572.35917, saving model to capsnet.hdf5\n",
      "2000/2000 [==============================] - 2060s 1s/step - loss: 397023.2420 - capsnet_loss: 0.8084 - decoder_loss: 26468.1620 - capsnet_acc: 0.6710 - val_loss: 359572.3592 - val_capsnet_loss: 0.8100 - val_decoder_loss: 23971.4365 - val_capsnet_acc: 0.5200\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984/2000 [============================>.] - ETA: 35:59 - loss: 352548.7812 - capsnet_loss: 0.8100 - decoder_loss: 23503.1973 - capsnet_acc: 0.750 - ETA: 35:03 - loss: 374433.2969 - capsnet_loss: 0.8100 - decoder_loss: 24962.1650 - capsnet_acc: 0.687 - ETA: 34:10 - loss: 388639.4792 - capsnet_loss: 0.8100 - decoder_loss: 25909.2441 - capsnet_acc: 0.656 - ETA: 33:20 - loss: 396978.4766 - capsnet_loss: 0.8100 - decoder_loss: 26465.1772 - capsnet_acc: 0.695 - ETA: 32:22 - loss: 395209.6375 - capsnet_loss: 0.8100 - decoder_loss: 26347.2547 - capsnet_acc: 0.693 - ETA: 31:35 - loss: 398611.1094 - capsnet_loss: 0.8100 - decoder_loss: 26574.0195 - capsnet_acc: 0.692 - ETA: 30:51 - loss: 397205.3527 - capsnet_loss: 0.8100 - decoder_loss: 26480.3025 - capsnet_acc: 0.687 - ETA: 30:08 - loss: 396823.7656 - capsnet_loss: 0.8100 - decoder_loss: 26454.8633 - capsnet_acc: 0.687 - ETA: 29:31 - loss: 394865.3194 - capsnet_loss: 0.8100 - decoder_loss: 26324.3003 - capsnet_acc: 0.694 - ETA: 28:52 - loss: 400372.7875 - capsnet_loss: 0.8100 - decoder_loss: 26691.4648 - capsnet_acc: 0.687 - ETA: 28:14 - loss: 398801.0114 - capsnet_loss: 0.8100 - decoder_loss: 26586.6797 - capsnet_acc: 0.673 - ETA: 27:38 - loss: 397506.9635 - capsnet_loss: 0.8100 - decoder_loss: 26500.4098 - capsnet_acc: 0.679 - ETA: 27:03 - loss: 394032.2019 - capsnet_loss: 0.8100 - decoder_loss: 26268.7590 - capsnet_acc: 0.689 - ETA: 26:28 - loss: 394301.4219 - capsnet_loss: 0.8100 - decoder_loss: 26286.7070 - capsnet_acc: 0.689 - ETA: 25:53 - loss: 395843.2000 - capsnet_loss: 0.8100 - decoder_loss: 26389.4923 - capsnet_acc: 0.683 - ETA: 25:20 - loss: 396598.8164 - capsnet_loss: 0.8100 - decoder_loss: 26439.8667 - capsnet_acc: 0.681 - ETA: 24:49 - loss: 397207.1232 - capsnet_loss: 0.8100 - decoder_loss: 26480.4205 - capsnet_acc: 0.667 - ETA: 24:19 - loss: 396556.2431 - capsnet_loss: 0.8100 - decoder_loss: 26437.0285 - capsnet_acc: 0.661 - ETA: 23:47 - loss: 394283.3109 - capsnet_loss: 0.8100 - decoder_loss: 26285.4997 - capsnet_acc: 0.659 - ETA: 23:14 - loss: 394465.7984 - capsnet_loss: 0.8100 - decoder_loss: 26297.6655 - capsnet_acc: 0.662 - ETA: 22:40 - loss: 394618.7158 - capsnet_loss: 0.8100 - decoder_loss: 26307.8600 - capsnet_acc: 0.660 - ETA: 22:08 - loss: 395791.9148 - capsnet_loss: 0.8100 - decoder_loss: 26386.0733 - capsnet_acc: 0.664 - ETA: 21:35 - loss: 396284.3356 - capsnet_loss: 0.8100 - decoder_loss: 26418.9014 - capsnet_acc: 0.672 - ETA: 21:03 - loss: 396882.9062 - capsnet_loss: 0.8100 - decoder_loss: 26458.8062 - capsnet_acc: 0.675 - ETA: 20:30 - loss: 396504.5513 - capsnet_loss: 0.8100 - decoder_loss: 26433.5825 - capsnet_acc: 0.680 - ETA: 19:56 - loss: 396488.2909 - capsnet_loss: 0.8100 - decoder_loss: 26432.4985 - capsnet_acc: 0.680 - ETA: 19:23 - loss: 397776.0891 - capsnet_loss: 0.8100 - decoder_loss: 26518.3517 - capsnet_acc: 0.677 - ETA: 18:49 - loss: 398854.0513 - capsnet_loss: 0.8100 - decoder_loss: 26590.2159 - capsnet_acc: 0.674 - ETA: 18:16 - loss: 399890.8890 - capsnet_loss: 0.8100 - decoder_loss: 26659.3384 - capsnet_acc: 0.673 - ETA: 17:42 - loss: 399906.9469 - capsnet_loss: 0.8100 - decoder_loss: 26660.4089 - capsnet_acc: 0.674 - ETA: 17:09 - loss: 399566.1754 - capsnet_loss: 0.8100 - decoder_loss: 26637.6908 - capsnet_acc: 0.674 - ETA: 16:35 - loss: 400998.1855 - capsnet_loss: 0.8100 - decoder_loss: 26733.1582 - capsnet_acc: 0.674 - ETA: 16:02 - loss: 399909.9848 - capsnet_loss: 0.8100 - decoder_loss: 26660.6115 - capsnet_acc: 0.675 - ETA: 15:29 - loss: 400924.3125 - capsnet_loss: 0.8100 - decoder_loss: 26728.2333 - capsnet_acc: 0.678 - ETA: 14:57 - loss: 399244.0848 - capsnet_loss: 0.8100 - decoder_loss: 26616.2181 - capsnet_acc: 0.675 - ETA: 14:24 - loss: 399355.8151 - capsnet_loss: 0.8100 - decoder_loss: 26623.6668 - capsnet_acc: 0.678 - ETA: 13:52 - loss: 399019.5034 - capsnet_loss: 0.8100 - decoder_loss: 26601.2460 - capsnet_acc: 0.677 - ETA: 13:19 - loss: 399544.2812 - capsnet_loss: 0.8100 - decoder_loss: 26636.2312 - capsnet_acc: 0.678 - ETA: 12:45 - loss: 399231.7468 - capsnet_loss: 0.8100 - decoder_loss: 26615.3956 - capsnet_acc: 0.680 - ETA: 12:12 - loss: 399319.8961 - capsnet_loss: 0.8100 - decoder_loss: 26621.2723 - capsnet_acc: 0.683 - ETA: 11:40 - loss: 399748.8575 - capsnet_loss: 0.8100 - decoder_loss: 26649.8697 - capsnet_acc: 0.683 - ETA: 11:07 - loss: 399894.0432 - capsnet_loss: 0.8100 - decoder_loss: 26659.5487 - capsnet_acc: 0.680 - ETA: 10:34 - loss: 399725.0218 - capsnet_loss: 0.8100 - decoder_loss: 26648.2806 - capsnet_acc: 0.678 - ETA: 10:01 - loss: 399003.5724 - capsnet_loss: 0.8100 - decoder_loss: 26600.1840 - capsnet_acc: 0.678 - ETA: 9:29 - loss: 397928.5125 - capsnet_loss: 0.8100 - decoder_loss: 26528.5133 - capsnet_acc: 0.675 - ETA: 8:56 - loss: 398706.5326 - capsnet_loss: 0.8100 - decoder_loss: 26580.3813 - capsnet_acc: 0.67 - ETA: 8:23 - loss: 398431.9953 - capsnet_loss: 0.8100 - decoder_loss: 26562.0788 - capsnet_acc: 0.67 - ETA: 7:51 - loss: 398439.2624 - capsnet_loss: 0.8100 - decoder_loss: 26562.5633 - capsnet_acc: 0.67 - ETA: 7:19 - loss: 398201.4777 - capsnet_loss: 0.8100 - decoder_loss: 26546.7110 - capsnet_acc: 0.67 - ETA: 6:46 - loss: 398010.0556 - capsnet_loss: 0.8100 - decoder_loss: 26533.9495 - capsnet_acc: 0.67 - ETA: 6:14 - loss: 398107.0123 - capsnet_loss: 0.8100 - decoder_loss: 26540.4133 - capsnet_acc: 0.67 - ETA: 5:42 - loss: 397793.8203 - capsnet_loss: 0.8100 - decoder_loss: 26519.5338 - capsnet_acc: 0.67 - ETA: 5:09 - loss: 397774.0625 - capsnet_loss: 0.8100 - decoder_loss: 26518.2166 - capsnet_acc: 0.67 - ETA: 4:36 - loss: 397963.0995 - capsnet_loss: 0.8100 - decoder_loss: 26530.8191 - capsnet_acc: 0.67 - ETA: 4:04 - loss: 397730.3937 - capsnet_loss: 0.8100 - decoder_loss: 26515.3054 - capsnet_acc: 0.67 - ETA: 3:31 - loss: 397483.3170 - capsnet_loss: 0.8100 - decoder_loss: 26498.8336 - capsnet_acc: 0.67 - ETA: 2:58 - loss: 397126.2988 - capsnet_loss: 0.8100 - decoder_loss: 26475.0324 - capsnet_acc: 0.67 - ETA: 2:26 - loss: 396908.9677 - capsnet_loss: 0.8100 - decoder_loss: 26460.5436 - capsnet_acc: 0.67 - ETA: 1:53 - loss: 397104.1600 - capsnet_loss: 0.8100 - decoder_loss: 26473.5565 - capsnet_acc: 0.66 - ETA: 1:21 - loss: 396886.9292 - capsnet_loss: 0.8100 - decoder_loss: 26459.0744 - capsnet_acc: 0.66 - ETA: 48s - loss: 396754.0476 - capsnet_loss: 0.8100 - decoder_loss: 26450.2156 - capsnet_acc: 0.6696 - ETA: 16s - loss: 396658.3352 - capsnet_loss: 0.8100 - decoder_loss: 26443.8348 - capsnet_acc: 0.6683Epoch 00002: val_loss did not improve\n",
      "2000/2000 [==============================] - 2090s 1s/step - loss: 396658.1192 - capsnet_loss: 0.8100 - decoder_loss: 26443.8204 - capsnet_acc: 0.6685 - val_loss: 359572.3592 - val_capsnet_loss: 0.8100 - val_decoder_loss: 23971.4365 - val_capsnet_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_data = get_tensors(train_files)\n",
    "valid_data = get_tensors(valid_files)\n",
    "\n",
    "train_model, eval_model = form_capsnet_architecture(input_shape=train_data.shape[1:], num_class=len(diseases),\n",
    "                                                    number_of_routing_iterations=3)\n",
    "checkpointer = ModelCheckpoint(filepath = 'capsnet.hdf5', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "train_model.compile(optimizer= 'adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.0005 * np.prod(train_data.shape[1:])], # Because we are using MSE insteaad of SSE \n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "history = train_model.fit([train_data, train_targets],[train_targets,train_data], epochs=2,\n",
    "                                validation_data=[[valid_data, valid_targets],[valid_targets, valid_data]], callbacks=[checkpointer], verbose=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model.compile(optimizer= 'adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.0005 * np.prod(train_data.shape[1:])],\n",
    "                  metrics={'capsnet': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10656744d2d245a996ea4a04a50189de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test acc: 0.648333333333\n"
     ]
    }
   ],
   "source": [
    "test_data = get_tensors(test_files)\n",
    "y_pred, ignore = eval_model.predict([test_data])\n",
    "print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(test_targets, 1))/test_targets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
