{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to laod datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the dataset\n",
    "\n",
    "First load the training, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, train_targets = load_dataset('data_clean/train')\n",
    "valid_files, valid_targets = load_dataset('data_clean/valid')\n",
    "test_files, test_targets = load_dataset('data_clean/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 classes: melanoma, nevus, seborrheic_keratosis.\n",
      "There are 2000 training images.\n",
      "There are 150 validation images.\n",
      "There are 600 testing images.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "diseases = sorted(listdir('data/train'))\n",
    "\n",
    "print('There are {} classes: {}.'.format(len(diseases), ', '.join(diseases)))\n",
    "print('There are {} training images.'.format(len(train_files)))\n",
    "print('There are {} validation images.'.format(len(valid_files)))\n",
    "print('There are {} testing images.'.format(len(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def get_tensor(path):\n",
    "    img = image.load_img(path,target_size=(100,100))\n",
    "    return np.expand_dims(image.img_to_array(img), axis=0)\n",
    "\n",
    "def get_tensors(paths):\n",
    "    return np.vstack([get_tensor(path) for path in tqdm_notebook(paths)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the Capsule Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers, models, optimizers\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCapsule(input, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap')(input)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_initializer='glorot_uniform', **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "\n",
    "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # The first two dimensions as `batch` dimension,\n",
    "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_capsnet_architecture(input_shape, num_class, number_of_routing_iterations):\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: ReLU Conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Primary Caps\n",
    "    primarycaps = PrimaryCapsule(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule Layer for outputs where dynamic routing works\n",
    "    tumorcaps = CapsuleLayer(num_capsule=num_class, dim_capsule=16, routings=number_of_routing_iterations,\n",
    "                             name='tumorcaps')(primarycaps)\n",
    "    \n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(tumorcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(num_class,))\n",
    "    masked_by_y = Mask()([tumorcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(tumorcaps)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(512, activation='relu', input_dim=16*num_class))\n",
    "    decoder.add(layers.Dense(1024, activation='relu'))\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "    \n",
    "    return train_model, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1b215993d948c4924d5aa891f844e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b530cd02794e20a20332300d865cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/2\n",
      "1984/2000 [============================>.] - ETA: 51:03 - loss: 26513.7109 - capsnet_loss: 0.7109 - decoder_loss: 26513.0000 - capsnet_acc: 0.21 - ETA: 49:03 - loss: 25839.1895 - capsnet_loss: 0.7514 - decoder_loss: 25838.4385 - capsnet_acc: 0.15 - ETA: 47:51 - loss: 26505.8789 - capsnet_loss: 0.7704 - decoder_loss: 26505.1087 - capsnet_acc: 0.33 - ETA: 46:36 - loss: 26905.5317 - capsnet_loss: 0.7802 - decoder_loss: 26904.7515 - capsnet_acc: 0.41 - ETA: 45:21 - loss: 27185.9313 - capsnet_loss: 0.7862 - decoder_loss: 27185.1449 - capsnet_acc: 0.46 - ETA: 44:11 - loss: 27058.8812 - capsnet_loss: 0.7902 - decoder_loss: 27058.0908 - capsnet_acc: 0.47 - ETA: 43:20 - loss: 27146.6222 - capsnet_loss: 0.7930 - decoder_loss: 27145.8290 - capsnet_acc: 0.49 - ETA: 42:36 - loss: 27266.1370 - capsnet_loss: 0.7951 - decoder_loss: 27265.3416 - capsnet_acc: 0.52 - ETA: 41:51 - loss: 26894.3140 - capsnet_loss: 0.7968 - decoder_loss: 26893.5169 - capsnet_acc: 0.53 - ETA: 40:59 - loss: 26561.4930 - capsnet_loss: 0.7981 - decoder_loss: 26560.6945 - capsnet_acc: 0.55 - ETA: 40:05 - loss: 26716.5920 - capsnet_loss: 0.7992 - decoder_loss: 26715.7924 - capsnet_acc: 0.56 - ETA: 39:11 - loss: 26722.5841 - capsnet_loss: 0.8001 - decoder_loss: 26721.7837 - capsnet_acc: 0.57 - ETA: 38:22 - loss: 26446.8230 - capsnet_loss: 0.8008 - decoder_loss: 26446.0218 - capsnet_acc: 0.58 - ETA: 37:38 - loss: 26284.5068 - capsnet_loss: 0.8015 - decoder_loss: 26283.7049 - capsnet_acc: 0.59 - ETA: 36:53 - loss: 26141.9589 - capsnet_loss: 0.8020 - decoder_loss: 26141.1564 - capsnet_acc: 0.60 - ETA: 36:03 - loss: 26258.1279 - capsnet_loss: 0.8025 - decoder_loss: 26257.3250 - capsnet_acc: 0.60 - ETA: 35:13 - loss: 26148.8598 - capsnet_loss: 0.8030 - decoder_loss: 26148.0564 - capsnet_acc: 0.61 - ETA: 33:57 - loss: 26154.8641 - capsnet_loss: 0.8034 - decoder_loss: 26154.0603 - capsnet_acc: 0.61 - ETA: 32:39 - loss: 26051.1391 - capsnet_loss: 0.8037 - decoder_loss: 26050.3349 - capsnet_acc: 0.61 - ETA: 31:26 - loss: 26088.1161 - capsnet_loss: 0.8040 - decoder_loss: 26087.3116 - capsnet_acc: 0.62 - ETA: 30:17 - loss: 26135.5158 - capsnet_loss: 0.8043 - decoder_loss: 26134.7110 - capsnet_acc: 0.62 - ETA: 29:11 - loss: 26185.0759 - capsnet_loss: 0.8046 - decoder_loss: 26184.2709 - capsnet_acc: 0.62 - ETA: 28:07 - loss: 26335.8280 - capsnet_loss: 0.8048 - decoder_loss: 26335.0227 - capsnet_acc: 0.63 - ETA: 27:07 - loss: 26351.7007 - capsnet_loss: 0.8050 - decoder_loss: 26350.8952 - capsnet_acc: 0.62 - ETA: 26:09 - loss: 26345.8034 - capsnet_loss: 0.8052 - decoder_loss: 26344.9977 - capsnet_acc: 0.63 - ETA: 25:12 - loss: 26372.6765 - capsnet_loss: 0.8054 - decoder_loss: 26371.8706 - capsnet_acc: 0.63 - ETA: 24:18 - loss: 26325.6121 - capsnet_loss: 0.8056 - decoder_loss: 26324.8060 - capsnet_acc: 0.63 - ETA: 23:25 - loss: 26244.4049 - capsnet_loss: 0.8057 - decoder_loss: 26243.5986 - capsnet_acc: 0.63 - ETA: 22:34 - loss: 26203.3929 - capsnet_loss: 0.8059 - decoder_loss: 26202.5865 - capsnet_acc: 0.64 - ETA: 21:44 - loss: 26151.3230 - capsnet_loss: 0.8060 - decoder_loss: 26150.5165 - capsnet_acc: 0.63 - ETA: 20:55 - loss: 26197.0848 - capsnet_loss: 0.8061 - decoder_loss: 26196.2782 - capsnet_acc: 0.63 - ETA: 20:07 - loss: 26297.9017 - capsnet_loss: 0.8063 - decoder_loss: 26297.0949 - capsnet_acc: 0.64 - ETA: 19:21 - loss: 26307.8249 - capsnet_loss: 0.8064 - decoder_loss: 26307.0181 - capsnet_acc: 0.64 - ETA: 18:35 - loss: 26347.0421 - capsnet_loss: 0.8065 - decoder_loss: 26346.2351 - capsnet_acc: 0.64 - ETA: 17:50 - loss: 26326.2384 - capsnet_loss: 0.8066 - decoder_loss: 26325.4313 - capsnet_acc: 0.65 - ETA: 17:06 - loss: 26368.7925 - capsnet_loss: 0.8067 - decoder_loss: 26367.9853 - capsnet_acc: 0.65 - ETA: 16:22 - loss: 26393.1032 - capsnet_loss: 0.8068 - decoder_loss: 26392.2959 - capsnet_acc: 0.65 - ETA: 15:39 - loss: 26431.8573 - capsnet_loss: 0.8069 - decoder_loss: 26431.0500 - capsnet_acc: 0.65 - ETA: 14:57 - loss: 26490.0117 - capsnet_loss: 0.8069 - decoder_loss: 26489.2042 - capsnet_acc: 0.65 - ETA: 14:15 - loss: 26487.4241 - capsnet_loss: 0.8070 - decoder_loss: 26486.6166 - capsnet_acc: 0.65 - ETA: 13:33 - loss: 26468.4449 - capsnet_loss: 0.8071 - decoder_loss: 26467.6373 - capsnet_acc: 0.65 - ETA: 12:53 - loss: 26448.5198 - capsnet_loss: 0.8072 - decoder_loss: 26447.7121 - capsnet_acc: 0.65 - ETA: 12:12 - loss: 26471.4768 - capsnet_loss: 0.8072 - decoder_loss: 26470.6691 - capsnet_acc: 0.65 - ETA: 11:32 - loss: 26461.2083 - capsnet_loss: 0.8073 - decoder_loss: 26460.4005 - capsnet_acc: 0.65 - ETA: 10:52 - loss: 26504.7450 - capsnet_loss: 0.8073 - decoder_loss: 26503.9372 - capsnet_acc: 0.65 - ETA: 10:13 - loss: 26509.3652 - capsnet_loss: 0.8074 - decoder_loss: 26508.5573 - capsnet_acc: 0.65 - ETA: 9:34 - loss: 26487.0144 - capsnet_loss: 0.8075 - decoder_loss: 26486.2064 - capsnet_acc: 0.6589 - ETA: 8:55 - loss: 26466.3905 - capsnet_loss: 0.8075 - decoder_loss: 26465.5825 - capsnet_acc: 0.659 - ETA: 8:17 - loss: 26443.1791 - capsnet_loss: 0.8076 - decoder_loss: 26442.3710 - capsnet_acc: 0.660 - ETA: 7:39 - loss: 26446.7548 - capsnet_loss: 0.8076 - decoder_loss: 26445.9467 - capsnet_acc: 0.660 - ETA: 7:01 - loss: 26481.2493 - capsnet_loss: 0.8077 - decoder_loss: 26480.4411 - capsnet_acc: 0.658 - ETA: 6:23 - loss: 26513.3213 - capsnet_loss: 0.8077 - decoder_loss: 26512.5131 - capsnet_acc: 0.656 - ETA: 5:46 - loss: 26502.5398 - capsnet_loss: 0.8077 - decoder_loss: 26501.7316 - capsnet_acc: 0.658 - ETA: 5:09 - loss: 26498.4397 - capsnet_loss: 0.8078 - decoder_loss: 26497.6314 - capsnet_acc: 0.659 - ETA: 4:32 - loss: 26485.3627 - capsnet_loss: 0.8078 - decoder_loss: 26484.5543 - capsnet_acc: 0.660 - ETA: 3:55 - loss: 26487.6702 - capsnet_loss: 0.8079 - decoder_loss: 26486.8618 - capsnet_acc: 0.661 - ETA: 3:18 - loss: 26453.5118 - capsnet_loss: 0.8079 - decoder_loss: 26452.7033 - capsnet_acc: 0.663 - ETA: 2:42 - loss: 26465.7669 - capsnet_loss: 0.8079 - decoder_loss: 26464.9584 - capsnet_acc: 0.665 - ETA: 2:05 - loss: 26490.2978 - capsnet_loss: 0.8080 - decoder_loss: 26489.4893 - capsnet_acc: 0.665 - ETA: 1:29 - loss: 26515.3626 - capsnet_loss: 0.8080 - decoder_loss: 26514.5541 - capsnet_acc: 0.668 - ETA: 53s - loss: 26484.9526 - capsnet_loss: 0.8080 - decoder_loss: 26484.1441 - capsnet_acc: 0.668 - ETA: 17s - loss: 26460.9292 - capsnet_loss: 0.8081 - decoder_loss: 26460.1206 - capsnet_acc: 0.6699Epoch 00001: val_loss improved from inf to 23972.24708, saving model to capsnet.hdf5\n",
      "2000/2000 [==============================] - 2282s 1s/step - loss: 26469.8323 - capsnet_loss: 0.8081 - decoder_loss: 26469.0237 - capsnet_acc: 0.6695 - val_loss: 23972.2471 - val_capsnet_loss: 0.8100 - val_decoder_loss: 23971.4365 - val_capsnet_acc: 0.5200\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984/2000 [============================>.] - ETA: 32:24 - loss: 27177.2871 - capsnet_loss: 0.8100 - decoder_loss: 27176.4766 - capsnet_acc: 0.71 - ETA: 31:57 - loss: 26645.5303 - capsnet_loss: 0.8100 - decoder_loss: 26644.7197 - capsnet_acc: 0.70 - ETA: 31:32 - loss: 26247.4004 - capsnet_loss: 0.8100 - decoder_loss: 26246.5898 - capsnet_acc: 0.69 - ETA: 31:35 - loss: 25773.9287 - capsnet_loss: 0.8100 - decoder_loss: 25773.1182 - capsnet_acc: 0.65 - ETA: 31:03 - loss: 25942.2902 - capsnet_loss: 0.8100 - decoder_loss: 25941.4797 - capsnet_acc: 0.65 - ETA: 30:26 - loss: 26199.5182 - capsnet_loss: 0.8100 - decoder_loss: 26198.7077 - capsnet_acc: 0.67 - ETA: 29:52 - loss: 26214.1470 - capsnet_loss: 0.8100 - decoder_loss: 26213.3365 - capsnet_acc: 0.66 - ETA: 29:27 - loss: 26422.0405 - capsnet_loss: 0.8100 - decoder_loss: 26421.2300 - capsnet_acc: 0.68 - ETA: 29:00 - loss: 26247.8418 - capsnet_loss: 0.8100 - decoder_loss: 26247.0312 - capsnet_acc: 0.69 - ETA: 28:30 - loss: 26255.2348 - capsnet_loss: 0.8100 - decoder_loss: 26254.4242 - capsnet_acc: 0.69 - ETA: 27:53 - loss: 26222.3109 - capsnet_loss: 0.8100 - decoder_loss: 26221.5004 - capsnet_acc: 0.69 - ETA: 27:18 - loss: 26278.8428 - capsnet_loss: 0.8100 - decoder_loss: 26278.0322 - capsnet_acc: 0.70 - ETA: 26:43 - loss: 26384.6852 - capsnet_loss: 0.8100 - decoder_loss: 26383.8747 - capsnet_acc: 0.70 - ETA: 26:08 - loss: 26463.2737 - capsnet_loss: 0.8100 - decoder_loss: 26462.4632 - capsnet_acc: 0.70 - ETA: 25:34 - loss: 26424.9736 - capsnet_loss: 0.8100 - decoder_loss: 26424.1630 - capsnet_acc: 0.70 - ETA: 25:00 - loss: 26526.4669 - capsnet_loss: 0.8100 - decoder_loss: 26525.6564 - capsnet_acc: 0.70 - ETA: 24:27 - loss: 26583.8244 - capsnet_loss: 0.8100 - decoder_loss: 26583.0139 - capsnet_acc: 0.70 - ETA: 23:58 - loss: 26545.9198 - capsnet_loss: 0.8100 - decoder_loss: 26545.1093 - capsnet_acc: 0.70 - ETA: 23:27 - loss: 26444.7451 - capsnet_loss: 0.8100 - decoder_loss: 26443.9345 - capsnet_acc: 0.70 - ETA: 22:56 - loss: 26404.5242 - capsnet_loss: 0.8100 - decoder_loss: 26403.7137 - capsnet_acc: 0.69 - ETA: 22:24 - loss: 26233.9578 - capsnet_loss: 0.8100 - decoder_loss: 26233.1472 - capsnet_acc: 0.70 - ETA: 21:53 - loss: 26116.6159 - capsnet_loss: 0.8100 - decoder_loss: 26115.8054 - capsnet_acc: 0.70 - ETA: 21:19 - loss: 26057.9410 - capsnet_loss: 0.8100 - decoder_loss: 26057.1304 - capsnet_acc: 0.71 - ETA: 20:46 - loss: 26059.9345 - capsnet_loss: 0.8100 - decoder_loss: 26059.1239 - capsnet_acc: 0.71 - ETA: 20:12 - loss: 26149.9981 - capsnet_loss: 0.8100 - decoder_loss: 26149.1876 - capsnet_acc: 0.70 - ETA: 19:39 - loss: 26140.3000 - capsnet_loss: 0.8100 - decoder_loss: 26139.4894 - capsnet_acc: 0.70 - ETA: 19:06 - loss: 26142.9864 - capsnet_loss: 0.8100 - decoder_loss: 26142.1759 - capsnet_acc: 0.69 - ETA: 18:33 - loss: 26160.9372 - capsnet_loss: 0.8100 - decoder_loss: 26160.1266 - capsnet_acc: 0.70 - ETA: 18:00 - loss: 26156.9366 - capsnet_loss: 0.8100 - decoder_loss: 26156.1260 - capsnet_acc: 0.70 - ETA: 17:28 - loss: 26188.5761 - capsnet_loss: 0.8100 - decoder_loss: 26187.7656 - capsnet_acc: 0.70 - ETA: 16:55 - loss: 26192.9747 - capsnet_loss: 0.8100 - decoder_loss: 26192.1641 - capsnet_acc: 0.69 - ETA: 16:23 - loss: 26138.1629 - capsnet_loss: 0.8100 - decoder_loss: 26137.3524 - capsnet_acc: 0.69 - ETA: 15:50 - loss: 26178.1999 - capsnet_loss: 0.8100 - decoder_loss: 26177.3894 - capsnet_acc: 0.69 - ETA: 15:17 - loss: 26248.5537 - capsnet_loss: 0.8100 - decoder_loss: 26247.7432 - capsnet_acc: 0.69 - ETA: 14:45 - loss: 26316.9157 - capsnet_loss: 0.8100 - decoder_loss: 26316.1052 - capsnet_acc: 0.69 - ETA: 14:19 - loss: 26282.5322 - capsnet_loss: 0.8100 - decoder_loss: 26281.7217 - capsnet_acc: 0.69 - ETA: 13:46 - loss: 26331.1228 - capsnet_loss: 0.8100 - decoder_loss: 26330.3123 - capsnet_acc: 0.69 - ETA: 13:10 - loss: 26382.4887 - capsnet_loss: 0.8100 - decoder_loss: 26381.6782 - capsnet_acc: 0.68 - ETA: 12:34 - loss: 26396.7220 - capsnet_loss: 0.8100 - decoder_loss: 26395.9114 - capsnet_acc: 0.68 - ETA: 12:00 - loss: 26382.9935 - capsnet_loss: 0.8100 - decoder_loss: 26382.1830 - capsnet_acc: 0.68 - ETA: 11:27 - loss: 26374.2758 - capsnet_loss: 0.8100 - decoder_loss: 26373.4652 - capsnet_acc: 0.68 - ETA: 10:55 - loss: 26380.3191 - capsnet_loss: 0.8100 - decoder_loss: 26379.5086 - capsnet_acc: 0.68 - ETA: 10:23 - loss: 26413.4980 - capsnet_loss: 0.8100 - decoder_loss: 26412.6875 - capsnet_acc: 0.68 - ETA: 9:51 - loss: 26416.4438 - capsnet_loss: 0.8100 - decoder_loss: 26415.6332 - capsnet_acc: 0.6847 - ETA: 9:19 - loss: 26445.8929 - capsnet_loss: 0.8100 - decoder_loss: 26445.0823 - capsnet_acc: 0.686 - ETA: 8:47 - loss: 26464.1627 - capsnet_loss: 0.8100 - decoder_loss: 26463.3522 - capsnet_acc: 0.686 - ETA: 8:15 - loss: 26513.6098 - capsnet_loss: 0.8100 - decoder_loss: 26512.7992 - capsnet_acc: 0.689 - ETA: 7:43 - loss: 26532.5014 - capsnet_loss: 0.8100 - decoder_loss: 26531.6909 - capsnet_acc: 0.689 - ETA: 7:11 - loss: 26489.3296 - capsnet_loss: 0.8100 - decoder_loss: 26488.5190 - capsnet_acc: 0.688 - ETA: 6:39 - loss: 26479.7154 - capsnet_loss: 0.8100 - decoder_loss: 26478.9048 - capsnet_acc: 0.687 - ETA: 6:07 - loss: 26486.2197 - capsnet_loss: 0.8100 - decoder_loss: 26485.4092 - capsnet_acc: 0.686 - ETA: 5:35 - loss: 26458.8617 - capsnet_loss: 0.8100 - decoder_loss: 26458.0512 - capsnet_acc: 0.689 - ETA: 5:03 - loss: 26501.6094 - capsnet_loss: 0.8100 - decoder_loss: 26500.7989 - capsnet_acc: 0.690 - ETA: 4:31 - loss: 26466.3473 - capsnet_loss: 0.8100 - decoder_loss: 26465.5368 - capsnet_acc: 0.689 - ETA: 3:59 - loss: 26471.0772 - capsnet_loss: 0.8100 - decoder_loss: 26470.2667 - capsnet_acc: 0.690 - ETA: 3:27 - loss: 26480.2076 - capsnet_loss: 0.8100 - decoder_loss: 26479.3970 - capsnet_acc: 0.690 - ETA: 2:55 - loss: 26505.0637 - capsnet_loss: 0.8100 - decoder_loss: 26504.2532 - capsnet_acc: 0.688 - ETA: 2:23 - loss: 26481.6953 - capsnet_loss: 0.8100 - decoder_loss: 26480.8848 - capsnet_acc: 0.687 - ETA: 1:51 - loss: 26438.6252 - capsnet_loss: 0.8100 - decoder_loss: 26437.8147 - capsnet_acc: 0.687 - ETA: 1:19 - loss: 26449.8175 - capsnet_loss: 0.8100 - decoder_loss: 26449.0070 - capsnet_acc: 0.685 - ETA: 47s - loss: 26426.0148 - capsnet_loss: 0.8100 - decoder_loss: 26425.2042 - capsnet_acc: 0.686 - ETA: 15s - loss: 26438.4203 - capsnet_loss: 0.8100 - decoder_loss: 26437.6097 - capsnet_acc: 0.6865Epoch 00002: val_loss did not improve\n",
      "2000/2000 [==============================] - 2040s 1s/step - loss: 26444.6310 - capsnet_loss: 0.8100 - decoder_loss: 26443.8204 - capsnet_acc: 0.6860 - val_loss: 23972.2471 - val_capsnet_loss: 0.8100 - val_decoder_loss: 23971.4365 - val_capsnet_acc: 0.5200\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_data = get_tensors(train_files)\n",
    "valid_data = get_tensors(valid_files)\n",
    "\n",
    "train_model, eval_model = form_capsnet_architecture(input_shape=train_data.shape[1:], num_class=len(diseases),\n",
    "                                                    number_of_routing_iterations=3)\n",
    "checkpointer = ModelCheckpoint(filepath = 'capsnet.hdf5', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "train_model.compile(optimizer= 'adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  metrics={'capsnet': 'accuracy'})\n",
    "\n",
    "history = train_model.fit([train_data, train_targets],[train_targets,train_data], epochs=2,\n",
    "                                validation_data=[[valid_data, valid_targets],[valid_targets, valid_data]], callbacks=[checkpointer], verbose=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model.compile(optimizer= 'adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  metrics={'capsnet': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835f304b9eae45a99a59d6313b2a6216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test acc: 0.655\n"
     ]
    }
   ],
   "source": [
    "test_data = get_tensors(test_files)\n",
    "y_pred, ignore = eval_model.predict([test_data])\n",
    "print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(test_targets, 1))/test_targets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
